{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision.transforms import Resize, Grayscale, ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "def run_test_suite(test_class):\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
    "    unittest.TextTestRunner(verbosity=2).run(suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Dataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storm Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for storm data.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory containing storm data.\n",
    "        storm_id (str or list of str): Storm IDs for the dataset.\n",
    "        sequence_length (int): Length of sequences to extract.\n",
    "        split (str): Dataset split ('train' or 'test').\n",
    "        test_size (float): Proportion of data to use for testing (if split is 'train').\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (str): Root directory containing storm data.\n",
    "        sequence_length (int): Length of sequences to extract.\n",
    "        transform (torchvision.transforms.Compose): Image transformations.\n",
    "        storm_id (str or list of str): Storm IDs for the dataset.\n",
    "        sequences (list): List of sequences containing images, features, and labels.\n",
    "\n",
    "    Methods:\n",
    "        _load_and_process_data(): Loads and processes storm data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, storm_id, sequence_length=15, split='train', test_size=0.2):\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.storm_id = storm_id\n",
    "        self.sequences = []\n",
    "        self._load_and_process_data()\n",
    "\n",
    "        # Split the dataset into train and test sets\n",
    "        train_sequences, test_sequences = train_test_split(self.sequences, test_size=test_size, random_state=42)\n",
    "        self.sequences = train_sequences if split == 'train' else test_sequences\n",
    "\n",
    "    def _load_and_process_data(self):\n",
    "        \"\"\"\n",
    "        Load and process storm data, extracting sequences of images, features, and labels.\n",
    "        \"\"\"\n",
    "        time_features = []\n",
    "\n",
    "        storms = self.storm_id\n",
    "\n",
    "        for storm_id in storms:\n",
    "            storm_path = os.path.join(self.root_dir, storm_id)\n",
    "            all_files = os.listdir(storm_path)\n",
    "\n",
    "            temp_images = []\n",
    "            temp_features = []\n",
    "            temp_labels = []\n",
    "\n",
    "            for file in sorted(all_files):\n",
    "                if file.endswith('.jpg'):\n",
    "                    image_path = os.path.join(storm_path, file)\n",
    "                    image = Image.open(image_path)\n",
    "                    temp_images.append(self.transform(image))\n",
    "                elif file.endswith('_features.json') or file.endswith('_label.json'):\n",
    "                    with open(os.path.join(storm_path, file), 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        if file.endswith('_features.json'):\n",
    "                            temp_features.append([float(data['relative_time']), float(data['ocean'])])\n",
    "                        else:\n",
    "                            temp_labels.append(float(data['wind_speed']))\n",
    "            max_relative_time = max([f[0] for f in temp_features])\n",
    "            for feature in temp_features:\n",
    "                feature[0] /= max_relative_time\n",
    "            time_features.extend([f[0] for f in temp_features])\n",
    "\n",
    "            for i in range(len(temp_images) - self.sequence_length):\n",
    "                self.sequences.append({\n",
    "                    'images': temp_images[i:i + self.sequence_length],\n",
    "                    'features': temp_features[i:i + self.sequence_length],\n",
    "                    'labels': temp_labels[i:i + self.sequence_length]\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        return {\n",
    "            'images': torch.stack(sequence['images']),\n",
    "            'features': torch.tensor(sequence['features'], dtype=torch.float),\n",
    "            'labels': torch.tensor(sequence['labels'], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channel = input_dim\n",
    "        self.hidden_channel = hidden_dim\n",
    "        self.kernel_sz = kernel_size\n",
    "        self.pad = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.use_bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_channel + self.hidden_channel,\n",
    "                              out_channels=4 * self.hidden_channel,\n",
    "                              kernel_size=self.kernel_sz,\n",
    "                              padding=self.pad,\n",
    "                              bias=self.use_bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_current, c_current = cur_state\n",
    "        combined = torch.cat([input_tensor, h_current], dim=1)\n",
    "        conv_result = self.conv(combined)\n",
    "        cc_inputgate, cc_forgetgate, cc_outputgate, cc_cellgate = torch.split(conv_result, self.hidden_channel, dim=1)\n",
    "        input_gate = torch.sigmoid(cc_inputgate)\n",
    "        forget_gate = torch.sigmoid(cc_forgetgate)\n",
    "        output_gate = torch.sigmoid(cc_outputgate)\n",
    "        cell_gate = torch.tanh(cc_cellgate)\n",
    "        c_next = forget_gate * c_current + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class StormGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StormGenerator, self).__init__()\n",
    "        self.encoder = SimpleCNN()\n",
    "        self.conv_lstm = ConvLSTM(input_dim=128, hidden_dim=[64, 32], kernel_size=(3, 3), num_layers=2, batch_first=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "    def forward(self, input_imgs):\n",
    "        batch_size, sequence_len, c, h, w = input_imgs.size()\n",
    "        # 修改这里，使用 .reshape() 而不是 .view()\n",
    "        c_input = input_imgs.reshape(batch_size * sequence_len, c, h, w)\n",
    "        c_output = self.encoder(c_input)\n",
    "        # 注意这里可能也需要使用 .reshape()，取决于后续操作\n",
    "        c_output = c_output.view(batch_size, sequence_len, -1, h // 8, w // 8)\n",
    "        conv_lstm_out, _ = self.conv_lstm(c_output)\n",
    "        conv_lstm_out = conv_lstm_out[0][:, -1, :, :, :]\n",
    "        output_image = self.decoder(conv_lstm_out)\n",
    "        return output_image\n",
    "\n",
    "\n",
    "model = StormGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve a list of storm IDs from a specified directory\n",
    "def get_storm_ids(root_dir):\n",
    "    \"\"\"\n",
    "    Get a list of storm IDs from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory containing storm data folders.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of storm IDs.\n",
    "    \"\"\"\n",
    "    storm_ids = [name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))]\n",
    "    return storm_ids\n",
    "\n",
    "# Set the root directory for storm data\n",
    "root_dir = '/Users/mk1923/Downloads/Selected_Storms_curated_to_zip'\n",
    "\n",
    "# Initialize the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the list of storm IDs using the get_storm_ids function\n",
    "storms = get_storm_ids(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve a list of storm IDs from a specified directory\n",
    "def get_storm_ids(root_dir):\n",
    "    \"\"\"\n",
    "    Get a list of storm IDs from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory containing storm data folders.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of storm IDs.\n",
    "    \"\"\"\n",
    "    storm_ids = [name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))]\n",
    "    return storm_ids\n",
    "\n",
    "# Set the root directory for storm data\n",
    "root_dir = '/Users/mk1923/Downloads/Selected_Storms_curated_to_zip'\n",
    "\n",
    "# Initialize the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the list of storm IDs using the get_storm_ids function\n",
    "storms = get_storm_ids(root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_data_transformation_effectiveness (__main__.StormDatasetTest.test_data_transformation_effectiveness) ... ok\n",
      "test_dataset_length_and_indexing (__main__.StormDatasetTest.test_dataset_length_and_indexing) ... ok\n",
      "test_dataset_loading (__main__.StormDatasetTest.test_dataset_loading) ... ok\n",
      "test_feature_normalization (__main__.StormDatasetTest.test_feature_normalization) ... ok\n",
      "test_image_normalization (__main__.StormDatasetTest.test_image_normalization) ... ok\n",
      "test_image_transformations (__main__.StormDatasetTest.test_image_transformations) ... ok\n",
      "test_label_processing (__main__.StormDatasetTest.test_label_processing) ... ok\n",
      "test_sequence_continuity (__main__.StormDatasetTest.test_sequence_continuity) ... ok\n",
      "test_unique_storms_count (__main__.StormDatasetTest.test_unique_storms_count) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 10.254s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class StormDatasetTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.root_dir = '/Users/mk1923/Downloads/Selected_Storms_curated_to_zip'\n",
    "        # Dynamically retrieve storm IDs\n",
    "        self.storm_ids = get_storm_ids(self.root_dir)  # Using the function to get storm IDs\n",
    "        self.sequence_length = 11\n",
    "\n",
    "        # Testing with the first retrieved storm ID for simplicity; expand as needed\n",
    "        storm_id_to_test = self.storm_ids[:1] if self.storm_ids else ['blq']\n",
    "\n",
    "        # Create the dataset\n",
    "        self.dataset = StormDataset(root_dir=self.root_dir, storm_id=storm_id_to_test, sequence_length=self.sequence_length, split='train')\n",
    "\n",
    "        self.train_dataset = StormDataset(root_dir=self.root_dir, storm_id=storm_id_to_test, sequence_length=self.sequence_length, split='train')\n",
    "        self.test_dataset = StormDataset(root_dir=self.root_dir, storm_id=storm_id_to_test, sequence_length=self.sequence_length, split='test')\n",
    "\n",
    "\n",
    "    def test_dataset_loading(self):\n",
    "        # Test a few samples for shape checks to ensure broader coverage\n",
    "        for idx in range(min(len(self.dataset), 5)):  # Test up to 5 samples\n",
    "            sample = self.dataset[idx]\n",
    "            self.assertEqual(sample['images'].shape, (self.sequence_length, 1, 224, 224), \"Input sequence shape is incorrect for sample {}\".format(idx))\n",
    "            self.assertEqual(sample['features'].shape, (self.sequence_length, 2), \"Features shape is incorrect for sample {}\".format(idx))\n",
    "            self.assertEqual(sample['labels'].shape, (self.sequence_length,), \"Labels shape is incorrect for sample {}\".format(idx))\n",
    "\n",
    "    def test_unique_storms_count(self):\n",
    "        # Testing against dynamic retrieval of storm IDs\n",
    "        expected_count = len(self.storm_ids)\n",
    "        found_storms = len(set(self.storm_ids))  # Assuming each directory name is unique\n",
    "        self.assertEqual(expected_count, found_storms, \"The number of unique storms does not match the expected count\")\n",
    "\n",
    "    def test_image_transformations(self):\n",
    "        # Test the first image in the sequence for a few samples\n",
    "        for idx in range(min(len(self.dataset), 5)):  # Test up to 5 samples\n",
    "            sample = self.dataset[idx]\n",
    "            image = sample['images'][0]  # First image\n",
    "            self.assertEqual(image.shape[0], 1, \"Image should be converted to grayscale for sample {}\".format(idx))\n",
    "            self.assertEqual(image.shape[1:], (224, 224), \"Image size should be resized to 224x224 for sample {}\".format(idx))\n",
    "\n",
    "    def test_feature_normalization(self):\n",
    "        # Test feature normalization for a few samples\n",
    "        for idx in range(min(len(self.dataset), 5)):  # Test up to 5 samples\n",
    "            sample = self.dataset[idx]\n",
    "            features = sample['features']\n",
    "            normalized_values = features[:, 0]  # Assuming first feature is 'relative_time'\n",
    "            self.assertTrue(torch.all((0 <= normalized_values) & (normalized_values <= 1)), \"Feature values should be normalized between 0 and 1 for sample {}\".format(idx))\n",
    "\n",
    "    def test_label_processing(self):\n",
    "        # Test label processing for a few samples\n",
    "        for idx in range(min(len(self.dataset), 5)):  # Test up to 5 samples\n",
    "            sample = self.dataset[idx]\n",
    "            labels = sample['labels']\n",
    "            self.assertTrue(torch.is_tensor(labels), \"Labels should be a PyTorch tensor for sample {}\".format(idx))\n",
    "\n",
    "\n",
    "    def test_image_normalization(self):\n",
    "        # Test that image pixel values are normalized between 0 and 1.\n",
    "        sample = self.dataset[0]\n",
    "        images = sample['images']\n",
    "        self.assertTrue(torch.all(images <= 1) and torch.all(images >= 0), \"Image pixel values should be normalized between 0 and 1.\")\n",
    "\n",
    "    def test_sequence_continuity(self):\n",
    "        # Test for sequence continuity in time features, if applicable.\n",
    "        sample = self.dataset[0]\n",
    "        features = sample['features']\n",
    "        # Ensure continuity if your features include time as the first element.\n",
    "        time_differences = torch.diff(features[:, 0])\n",
    "        self.assertTrue(torch.all(time_differences > 0), \"Time features should be continuously increasing.\")\n",
    "\n",
    "    def test_data_transformation_effectiveness(self):\n",
    "        # Test the effectiveness of data transformations.\n",
    "        sample = self.dataset[0]\n",
    "        images = sample['images']\n",
    "        self.assertEqual(images.shape[1:], (1, 224, 224), \"Transformed images should have the correct shape.\")\n",
    "\n",
    "    def test_dataset_length_and_indexing(self):\n",
    "        # Test the reported length of the dataset and ability to index into it.\n",
    "        self.assertTrue(len(self.dataset) > 0, \"Dataset should report a non-zero length.\")\n",
    "        try:\n",
    "            sample = self.dataset[len(self.dataset) - 1]\n",
    "        except IndexError:\n",
    "            self.fail(\"Indexing the last element of the dataset raised an IndexError.\")\n",
    "\n",
    "\n",
    "\n",
    "# Run StormDatasetTest\n",
    "run_test_suite(StormDatasetTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "root_dir = '/Users/mk1923/Downloads/Selected_Storms_curated_to_zip'\n",
    "\n",
    "train_dataset = StormDataset(root_dir, storm_id=storms, sequence_length=11, split='train')\n",
    "val_dataset = StormDataset(root_dir, storm_id=storms, sequence_length=11, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Datalaoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_dataloader_output (__main__.DataLoaderTest) ... ok\n",
      "test_total_images_processed (__main__.DataLoaderTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 27.754s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class DataLoaderTest(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setup common parameters for tests\n",
    "        self.root_dir = '/content/drive/MyDrive/Selected_Storms_curated'\n",
    "        self.storm_id = ['bkh']\n",
    "        self.sequence_length = 11\n",
    "        self.batch_size = 32\n",
    "        self.train_dataset = StormDataset(root_dir=self.root_dir, storm_id=self.storm_id, sequence_length=self.sequence_length, split='train')\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def test_dataloader_output(self):\n",
    "        batch = next(iter(self.train_loader))\n",
    "        images, features, labels = batch['images'], batch['features'], batch['labels']\n",
    "\n",
    "        self.assertEqual(images.shape, (self.batch_size, self.sequence_length, 1, 224, 224), \"Batched images shape is incorrect\")\n",
    "        self.assertEqual(features.shape, (self.batch_size, self.sequence_length, 2), \"Batched features shape is incorrect\")\n",
    "        self.assertEqual(labels.shape, (self.batch_size, self.sequence_length), \"Batched labels shape is incorrect\")\n",
    "\n",
    "    def test_total_images_processed(self):\n",
    "        total_images = 0\n",
    "        for batch in self.train_loader:\n",
    "            total_images += batch['images'].size(0) * batch['images'].size(1)  # batch size * sequence length\n",
    "\n",
    "        expected_total_images = len(self.train_dataset) * self.sequence_length\n",
    "        self.assertEqual(total_images, expected_total_images, f\"Total processed images should be {expected_total_images}, got {total_images}\")\n",
    "\n",
    "# Run StormDatasetTest\n",
    "run_test_suite(DataLoaderTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channel = input_dim\n",
    "        self.hidden_channel = hidden_dim\n",
    "        self.kernel_sz = kernel_size\n",
    "        self.pad = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.use_bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_channel + self.hidden_channel,\n",
    "                              out_channels=4 * self.hidden_channel,\n",
    "                              kernel_size=self.kernel_sz,\n",
    "                              padding=self.pad,\n",
    "                              bias=self.use_bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_current, c_current = cur_state\n",
    "        combined = torch.cat([input_tensor, h_current], dim=1)\n",
    "        conv_result = self.conv(combined)\n",
    "        cc_inputgate, cc_forgetgate, cc_outputgate, cc_cellgate = torch.split(conv_result, self.hidden_channel, dim=1)\n",
    "        input_gate = torch.sigmoid(cc_inputgate)\n",
    "        forget_gate = torch.sigmoid(cc_forgetgate)\n",
    "        output_gate = torch.sigmoid(cc_outputgate)\n",
    "        cell_gate = torch.tanh(cc_cellgate)\n",
    "        c_next = forget_gate * c_current + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class StormGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StormGenerator, self).__init__()\n",
    "        self.encoder = SimpleCNN()\n",
    "        self.conv_lstm = ConvLSTM(input_dim=128, hidden_dim=[64, 32], kernel_size=(3, 3), num_layers=2, batch_first=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "    def forward(self, input_imgs):\n",
    "        batch_size, sequence_len, c, h, w = input_imgs.size()\n",
    "        # 修改这里，使用 .reshape() 而不是 .view()\n",
    "        c_input = input_imgs.reshape(batch_size * sequence_len, c, h, w)\n",
    "        c_output = self.encoder(c_input)\n",
    "        # 注意这里可能也需要使用 .reshape()，取决于后续操作\n",
    "        c_output = c_output.view(batch_size, sequence_len, -1, h // 8, w // 8)\n",
    "        conv_lstm_out, _ = self.conv_lstm(c_output)\n",
    "        conv_lstm_out = conv_lstm_out[0][:, -1, :, :, :]\n",
    "        output_image = self.decoder(conv_lstm_out)\n",
    "        return output_image\n",
    "\n",
    "\n",
    "model = StormGenerator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Model Archeticture Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_pass (__main__.TestModelArchitecture.test_forward_pass) ... ok\n",
      "test_gradient_calculation (__main__.TestModelArchitecture.test_gradient_calculation) ... ok\n",
      "test_initialization (__main__.TestModelArchitecture.test_initialization) ... ok\n",
      "test_input_type_handling (__main__.TestModelArchitecture.test_input_type_handling) ... ok\n",
      "test_layer_consistency (__main__.TestModelArchitecture.test_layer_consistency) ... ok\n",
      "test_matrix_shapes_compatibility (__main__.TestModelArchitecture.test_matrix_shapes_compatibility) ... ok\n",
      "test_simple_cnn_output_shape (__main__.TestModelArchitecture.test_simple_cnn_output_shape) ... ok\n",
      "test_simple_cnn_parameter_count (__main__.TestModelArchitecture.test_simple_cnn_parameter_count) ... ok\n",
      "test_simple_cnn_relu_activations (__main__.TestModelArchitecture.test_simple_cnn_relu_activations) ... ok\n",
      "test_storm_generator_output_range (__main__.TestModelArchitecture.test_storm_generator_output_range) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 9.751s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unittest\n",
    "import torch.nn as nn\n",
    "\n",
    "class TestModelArchitecture(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Set up common variables and model instances for tests\n",
    "        self.input_dim = 128\n",
    "        self.hidden_dim = [64, 32]\n",
    "        self.kernel_size = (3, 3)\n",
    "        self.num_layers = 2\n",
    "        self.batch_size = 1\n",
    "        self.seq_len = 5\n",
    "        self.channels = 1\n",
    "        self.height = self.width = 224  # Assuming square input images\n",
    "\n",
    "        # Initialize models\n",
    "        self.conv_lstm_cell = ConvLSTMCell(self.input_dim, self.hidden_dim[0], self.kernel_size, True)\n",
    "        self.conv_lstm = ConvLSTM(self.input_dim, self.hidden_dim, self.kernel_size, self.num_layers, batch_first=True)\n",
    "        self.simple_cnn = SimpleCNN()\n",
    "        self.storm_generator = StormGenerator()\n",
    "\n",
    "    def test_initialization(self):\n",
    "        # Test for ConvLSTMCell\n",
    "        self.assertIsInstance(self.conv_lstm_cell.conv, nn.Conv2d, \"ConvLSTMCell Conv2d initialization failed\")\n",
    "\n",
    "        # Test for ConvLSTM\n",
    "        self.assertEqual(len(self.conv_lstm.cell_list), self.num_layers, \"ConvLSTM cell_list initialization failed\")\n",
    "\n",
    "        # Test for SimpleCNN\n",
    "        self.assertTrue(isinstance(self.simple_cnn.conv_layers, nn.Sequential), \"SimpleCNN Sequential initialization failed\")\n",
    "\n",
    "        # Test for StormGenerator\n",
    "        self.assertTrue(isinstance(self.storm_generator.encoder, SimpleCNN), \"StormGenerator encoder initialization failed\")\n",
    "        self.assertTrue(isinstance(self.storm_generator.conv_lstm, ConvLSTM), \"StormGenerator ConvLSTM initialization failed\")\n",
    "        self.assertTrue(isinstance(self.storm_generator.decoder, nn.Sequential), \"StormGenerator decoder initialization failed\")\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        # Simulate input for forward pass\n",
    "        input_tensor = torch.randn(self.batch_size, self.seq_len, self.channels, self.height, self.width)\n",
    "        output = self.storm_generator(input_tensor)\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.channels, self.height, self.width), \"StormGenerator forward pass output shape is incorrect\")\n",
    "\n",
    "    def test_layer_consistency(self):\n",
    "        # Verifying the consistency of layers within ConvLSTM and SimpleCNN components\n",
    "\n",
    "        # SimpleCNN layer tests\n",
    "        conv_layers_count = sum(1 for _ in filter(lambda layer: isinstance(layer, nn.Conv2d), self.simple_cnn.conv_layers))\n",
    "        self.assertEqual(conv_layers_count, 3, \"SimpleCNN does not contain the expected number of Conv2d layers\")\n",
    "\n",
    "        # ConvLSTM layer tests\n",
    "        for i, cell in enumerate(self.conv_lstm.cell_list):\n",
    "            self.assertTrue(isinstance(cell, ConvLSTMCell), f\"Layer {i} in ConvLSTM is not a ConvLSTMCell\")\n",
    "            self.assertEqual(cell.kernel_sz, self.kernel_size, f\"ConvLSTMCell {i} kernel size is incorrect\")\n",
    "            self.assertTrue(cell.use_bias, f\"ConvLSTMCell {i} bias is not being used as expected\")\n",
    "\n",
    "    def test_matrix_shapes_compatibility(self):\n",
    "        model = StormGenerator()\n",
    "        mock_input = torch.randn(32, 11, 1, 224, 224)  # Adjust mock input to match your model's expected input shape\n",
    "        try:\n",
    "            model(mock_input)  # Perform a forward pass with mock input\n",
    "        except RuntimeError as e:\n",
    "            if \"size mismatch\" in str(e):\n",
    "                self.fail(f\"Matrix shape conflict encountered: {e}\")\n",
    "\n",
    "    def test_simple_cnn_output_shape(self):\n",
    "        input_tensor = torch.randn(self.batch_size, self.channels, self.height, self.width)\n",
    "        output = self.simple_cnn(input_tensor)\n",
    "        expected_output_shape = (self.batch_size, 128, self.height // 8, self.width // 8)  # Adjust based on expected output\n",
    "        self.assertEqual(output.shape, expected_output_shape, \"SimpleCNN output shape is incorrect\")\n",
    "\n",
    "    def test_simple_cnn_relu_activations(self):\n",
    "        input_tensor = torch.randn(self.batch_size, self.channels, self.height, self.width)\n",
    "        output = self.simple_cnn(input_tensor)\n",
    "        self.assertTrue(torch.all(output >= 0), \"SimpleCNN ReLU activations not applied correctly\")\n",
    "\n",
    "    def test_simple_cnn_parameter_count(self):\n",
    "        num_params = sum(p.numel() for p in self.simple_cnn.parameters())\n",
    "        expected_params = 92672  # Adjust based on the expected count\n",
    "        self.assertEqual(num_params, expected_params, \"SimpleCNN parameter count is incorrect\")\n",
    "\n",
    "    def test_storm_generator_output_range(self):\n",
    "        input_tensor = torch.randn(self.batch_size, self.seq_len, self.channels, self.height, self.width)\n",
    "        output = self.storm_generator(input_tensor)\n",
    "        self.assertTrue(torch.all(output >= -1) and torch.all(output <= 1), \"StormGenerator output range is incorrect\")\n",
    "\n",
    "\n",
    "    def test_gradient_calculation(self):\n",
    "        input_tensor = torch.randn(self.batch_size, self.seq_len, self.channels, self.height, self.width, requires_grad=True)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.storm_generator.parameters(), lr=0.01)\n",
    "\n",
    "        for _ in range(10):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.storm_generator(input_tensor)\n",
    "            loss = loss_fn(output, torch.randn(self.batch_size, self.channels, self.height, self.width))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self.assertTrue(all(param.grad is not None for param in self.storm_generator.parameters()), \"Gradient calculation failed\")\n",
    "\n",
    "    def test_input_type_handling(self):\n",
    "        input_tensor = torch.randn(self.batch_size, self.seq_len, self.channels, self.height, self.width, dtype=torch.float64)\n",
    "        output = self.storm_generator(input_tensor.float())  # Convert input tensor to float32\n",
    "        self.assertEqual(output.dtype, torch.float32, \"StormGenerator output dtype is incorrect\")\n",
    "\n",
    "\n",
    "\n",
    "# Run StormDatasetTest\n",
    "run_test_suite(TestModelArchitecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_msssim in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from pytorch_msssim) (2.1.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from torch->pytorch_msssim) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from jinja2->torch->pytorch_msssim) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_msssim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader, optimizer, device, num_epochs):\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_history = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(num_epochs):  # Assuming num_epochs is defined\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            images = batch['images'].to(device)  # Assuming images key in your dataset\n",
    "\n",
    "            # Split the images into input sequence and target image\n",
    "            input_images = images[:, :5, :, :, :]  # Use the first five images as input sequence\n",
    "            target_image = images[:, 5, :, :, :].squeeze(1)  # Use the sixth image as target\n",
    "            target_image = target_image.unsqueeze(1)  # Ensure target image has the channel dimension\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # The model expects a 5D tensor, so no change is needed here\n",
    "            predicted_image = model(input_images)\n",
    "\n",
    "            # Calculate loss using SSIM\n",
    "            loss = 1 - ssim(predicted_image, target_image, data_range=1, size_average=True)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)  # Store the average loss for this epoch\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    # After training, plot the training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSIM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StormGenerator().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "num_epochs = 1\n",
    "train(model, train_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                  ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^    self = reduction.pickle.load(from_parent)\n",
      "^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^\n",
      "^AttributeError: ^Can't get attribute 'StormDataset' on <module '__main__' (built-in)>^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StormDataset' on <module '__main__' (built-in)>\n",
      "ETraceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "                    ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "AttributeErrorAttributeError: : Can't get attribute 'StormDataset' on <module '__main__' (built-in)>Can't get attribute 'StormDataset' on <module '__main__' (built-in)>\n",
      "\n",
      "E...................E\n",
      "======================================================================\n",
      "ERROR: test_dataloader_output (__main__.DataLoaderTest.test_dataloader_output)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 256, in poll\n",
      "    return self._poll(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 423, in _poll\n",
      "    r = wait([self], timeout)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 46631) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3q/ddd0qxh91wscnmzslb2357qm0000gq/T/ipykernel_45241/4060872270.py\", line 12, in test_dataloader_output\n",
      "    batch = next(iter(self.train_loader))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1294, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1145, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 46631, 46632) exited unexpectedly\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_total_images_processed (__main__.DataLoaderTest.test_total_images_processed)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 256, in poll\n",
      "    return self._poll(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 423, in _poll\n",
      "    r = wait([self], timeout)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 46633) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3q/ddd0qxh91wscnmzslb2357qm0000gq/T/ipykernel_45241/4060872270.py\", line 21, in test_total_images_processed\n",
      "    for batch in self.train_loader:\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1294, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1145, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 46633) exited unexpectedly\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_train_initializes_loss_history_and_trains (__main__.TestTrainingFunction.test_train_initializes_loss_history_and_trains)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/unittest/mock.py\", line 1375, in patched\n",
      "    return func(*newargs, **newkeywargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/3q/ddd0qxh91wscnmzslb2357qm0000gq/T/ipykernel_45241/4242591116.py\", line 16, in test_train_initializes_loss_history_and_trains\n",
      "    mock_train_loader = StormDataset(root_dir='/Users/mk1923/Downloads/Selected_Storms_curated_to_zip', storm_id=mock_storm_id)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/3q/ddd0qxh91wscnmzslb2357qm0000gq/T/ipykernel_45241/3095963816.py\", line 33, in __init__\n",
      "    self._load_and_process_data()\n",
      "  File \"/var/folders/3q/ddd0qxh91wscnmzslb2357qm0000gq/T/ipykernel_45241/3095963816.py\", line 67, in _load_and_process_data\n",
      "    max_relative_time = max([f[0] for f in temp_features])\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: max() arg is an empty sequence\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 22 tests in 32.327s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import MagicMock, patch\n",
    "import torch\n",
    "\n",
    "class TestTrainingFunction(unittest.TestCase):\n",
    "    @patch('__main__.ssim', return_value=torch.tensor(0.8, requires_grad=True))  # Ensure SSIM returns a tensor with gradients\n",
    "    def test_train_initializes_loss_history_and_trains(self, mock_ssim):\n",
    "        mock_model = MagicMock()\n",
    "        mock_model.return_value = torch.randn(1, 3, 64, 64, requires_grad=True)  # Mock model output\n",
    "        \n",
    "        mock_optimizer = MagicMock()\n",
    "        mock_device = torch.device('cpu')\n",
    "        \n",
    "        # Provide a valid 'storm_id' corresponding to an existing directory within 'root_dir'\n",
    "        mock_storm_id = '/Users/mk1923/Downloads/Selected_Storms_curated_to_zip/blq'  # Replace with a valid storm ID\n",
    "        mock_train_loader = StormDataset(root_dir='/Users/mk1923/Downloads/Selected_Storms_curated_to_zip', storm_id=mock_storm_id)\n",
    "        \n",
    "        num_epochs = 1\n",
    "        \n",
    "        # Run the training function\n",
    "        with patch('__main__.plt.show'):\n",
    "            loss_history = train(mock_model, mock_train_loader, mock_optimizer, mock_device, num_epochs)\n",
    "        \n",
    "        # Assertions\n",
    "        self.assertIsInstance(loss_history, list)\n",
    "        self.assertGreater(len(loss_history), 0)\n",
    "\n",
    "\n",
    "# Running the test without using unittest.main() to avoid argv issues in Jupyter\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
